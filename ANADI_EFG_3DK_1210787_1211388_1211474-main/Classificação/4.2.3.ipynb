{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RC7P2lVLUqsZ",
        "outputId": "ccc8e14e-92fa-4767-bede-2c6ebb5dda1e"
      },
      "outputs": [],
      "source": [
        "#melhor redes ADAM normal\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "from sklearn.metrics import recall_score, f1_score, confusion_matrix\n",
        "\n",
        "epsilon = 1e-10\n",
        "# Carregar o conjunto de dados\n",
        "df = pd.read_csv('Dados_Trabalho_TP2.csv')\n",
        "\n",
        "# Pré-processar os dados\n",
        "le = LabelEncoder()\n",
        "for column in df.columns:\n",
        "    if df[column].dtype == object:\n",
        "        df[column] = le.fit_transform(df[column])\n",
        "\n",
        "# Dividir o conjunto de dados em recursos (X) e rótulos (y)\n",
        "X = df.drop('Genero', axis=1)\n",
        "y = df['Genero']\n",
        "\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "# Dividir o conjunto de dados em treinamento e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# np.random.seed(0)\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "optimizers = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
        "initializers = ['uniform', 'lecun_uniform', 'normal', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
        "\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "f1score_per_fold = []\n",
        "fold_no = 1\n",
        "epochs_list = [10, 20, 30]\n",
        "batch_sizes = [16, 32, 64]\n",
        "results = []\n",
        "\n",
        "for optimizer in optimizers:\n",
        "    for initializer in initializers:\n",
        "        for train, test in kfold.split(X_train, y_train):\n",
        "            model3 = tf.keras.Sequential([\n",
        "            tf.keras.Input(shape=(17,)),\n",
        "            tf.keras.layers.Dense(units=64, activation=\"relu\", kernel_initializer=initializer),\n",
        "            tf.keras.layers.Dense(units=48, activation=\"relu\", kernel_initializer=initializer),\n",
        "            tf.keras.layers.Dense(units=32, activation=\"relu\", kernel_initializer=initializer),\n",
        "            tf.keras.layers.Dense(units=32, activation=\"relu\", kernel_initializer=initializer),\n",
        "            tf.keras.layers.Dense(units=16, activation=\"relu\", kernel_initializer=initializer),\n",
        "            tf.keras.layers.Dense(units=16, activation=\"relu\", kernel_initializer=initializer),\n",
        "            tf.keras.layers.Dense(units=16, activation=\"relu\", kernel_initializer=initializer),\n",
        "            tf.keras.layers.Dense(units=16, activation=\"relu\", kernel_initializer=initializer),\n",
        "            tf.keras.layers.Dense(units=16, activation=\"relu\", kernel_initializer=initializer),\n",
        "            tf.keras.layers.Dense(units=16, activation=\"relu\", kernel_initializer=initializer),\n",
        "            tf.keras.layers.Dense(units=16, activation=\"relu\", kernel_initializer=initializer),\n",
        "            tf.keras.layers.Dense(units=9, activation=\"softmax\", kernel_initializer=initializer)\n",
        "        ])\n",
        "\n",
        "        model3.compile(optimizer=optimizer,\n",
        "                      loss='sparse_categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        # Treina o modelo\n",
        "        history4 = model3.fit(X_train, y_train, epochs=100, verbose=0)\n",
        "\n",
        "        # Avalia o modelo com métricas de generalização\n",
        "        scores3 = model3.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "        # Faz previsões no conjunto de teste\n",
        "        y_pred = model3.predict(X_test)\n",
        "        y_pred = np.argmax(y_pred, axis=1)  # Converte as previsões de one-hot para rótulos\n",
        "\n",
        "        # Calcula Sensitivity (Recall), Specificity e F1\n",
        "        sensitivity = recall_score(y_test, y_pred, average='macro')\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        specificity = cm[0,0] / (cm[0,0] + cm[0,1] + epsilon)\n",
        "        f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "        # Armazena os resultados\n",
        "        results.append({\n",
        "            'optimizer': optimizer,\n",
        "            'initializer': initializer,\n",
        "            'accuracy': scores3[1] * 100,\n",
        "            'loss': scores3[0],\n",
        "            'sensitivity': sensitivity,\n",
        "            'specificity': specificity,\n",
        "            'f1': f1\n",
        "        })\n",
        "\n",
        "# Encontra o resultado com a maior acurácia\n",
        "best_result = max(results, key=lambda x: x['accuracy'])\n",
        "\n",
        "print('Melhor acurácia:', best_result['accuracy'])\n",
        "print('Melhor otimizador:', best_result['optimizer'])\n",
        "print('Melhor inicializador:', best_result['initializer'])\n",
        "print('Sensibilidade:', best_result['sensitivity']*100)\n",
        "print('Especificidade:', best_result['specificity']*100)\n",
        "print('F1:', best_result['f1']*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7DHj945bve0",
        "outputId": "2ea89b84-e4df-42d5-b108-bd9e51e94465"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Carregar o conjunto de dados\n",
        "df = pd.read_csv('Dados_Trabalho_TP2.csv')\n",
        "\n",
        "# Pré-processar os dados\n",
        "le = LabelEncoder()\n",
        "for column in df.columns:\n",
        "    if df[column].dtype == object:\n",
        "        df[column] = le.fit_transform(df[column])\n",
        "\n",
        "# Dividir o conjunto de dados em recursos (X) e rótulos (y)\n",
        "X = df.drop('Genero', axis=1)\n",
        "y = df['Genero']\n",
        "\n",
        "# Normalizar os dados\n",
        "scaler = StandardScaler()\n",
        "X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "\n",
        "# Dividir o conjunto de dados em treinamento e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Definir o número de folds\n",
        "k = 5\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "# Definindo o modelo SVM com kernel RBF\n",
        "kernel = 'rbf'\n",
        "svm = SVC(kernel=kernel)\n",
        "\n",
        "# Ajuste de parâmetros utilizando GridSearchCV para evitar overfitting\n",
        "param_grid = {'C': [0.1, 1, 10, 100, 1000], 'gamma': [10, 'scale', 'auto']}\n",
        "grid = GridSearchCV(svm, param_grid, cv=kf, refit=True, scoring='accuracy', n_jobs=-1)\n",
        "grid.fit(X, y)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "# Avaliar o modelo usando k-fold cross validation\n",
        "scores = []\n",
        "sensitivities = []\n",
        "specificities = []\n",
        "f1_scores = []\n",
        "epsilon = 1e-7  # para evitar divisão por zero\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    best_model.fit(X_train, y_train)\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    fold_score = accuracy_score(y_test, y_pred)\n",
        "    scores.append(fold_score)\n",
        "\n",
        "    # Calcular a sensibilidade (recall), especificidade e F1 para este fold\n",
        "    sensitivity = recall_score(y_test, y_pred, average='macro')\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    specificity = cm[0,0] / (cm[0,0] + cm[0,1] + epsilon)\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "    # Adicionar as métricas deste fold às listas correspondentes\n",
        "    sensitivities.append(sensitivity)\n",
        "    specificities.append(specificity)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "# Calcular a média e desvio padrão das acurácias\n",
        "mean_score = np.mean(scores) * 100\n",
        "std_score = np.std(scores)\n",
        "\n",
        "# Calcular a média das sensibilidades, especificidades e F1\n",
        "mean_sensitivity = np.mean(sensitivities)\n",
        "mean_specificity = np.mean(specificities)\n",
        "mean_f1 = np.mean(f1_scores)\n",
        "\n",
        "# Armazenar os melhores resultados\n",
        "best_result = {\n",
        "    'accuracy': mean_score,\n",
        "    'optimizer': grid.best_params_['C'],\n",
        "    'initializer': grid.best_params_['gamma'],\n",
        "    'sensitivity': mean_sensitivity,\n",
        "    'specificity': mean_specificity,\n",
        "    'f1': mean_f1\n",
        "}\n",
        "\n",
        "# Imprimir os melhores resultados\n",
        "print('Melhor acurácia:', best_result['accuracy'])\n",
        "print('Melhor otimizador:', best_result['optimizer'])\n",
        "print('Melhor inicializador:', best_result['initializer'])\n",
        "print('Sensibilidade:', best_result['sensitivity']*100)\n",
        "print('Especificidade:', best_result['specificity']*100)\n",
        "print('F1:', best_result['f1']*100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8_RPZ2-dnfm",
        "outputId": "7ef4dd61-f694-4434-8f5f-c20e502ea62d"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "import numpy as np\n",
        "\n",
        "# Pontuações do modelo SVM\n",
        "scores_svm = scores\n",
        "\n",
        "# Imprime as pontuações de acurácia do modelo SVM\n",
        "print(f\"Pontuações de acurácia do Modelo SVM: {scores_svm}\")\n",
        "\n",
        "# Extrai as pontuações de acurácia da lista 'results'\n",
        "scores_model3 = [result['accuracy'] for result in results]\n",
        "\n",
        "# Imprime as pontuações de acurácia do modelo 3\n",
        "print(f\"Pontuações de acurácia do Modelo 3: {scores_model3}\")\n",
        "\n",
        "# Realiza o teste t\n",
        "t_stat_svm_3, p_val_svm_3 = stats.ttest_ind(scores_svm, scores_model3)\n",
        "\n",
        "# Imprime os resultados\n",
        "print(f\"Modelo SVM vs Modelo 3: t statistic: {t_stat_svm_3:.2f}, p value: {p_val_svm_3:.12f}\")\n",
        "\n",
        "# Verifica se a diferença é significativa\n",
        "if p_val_svm_3 < 0.05:\n",
        "    print(\"Há uma diferença significativa no desempenho dos dois modelos.\")\n",
        "else:\n",
        "    print(\"Não há uma diferença significativa no desempenho dos dois modelos.\")\n",
        "\n",
        "# Identifica o modelo com melhor desempenho\n",
        "if np.mean(scores_svm) > np.mean(scores_model3):\n",
        "    print(\"O Modelo SVM tem melhor desempenho.\")\n",
        "else:\n",
        "    print(\"O Modelo 3 (Rede Neural) tem melhor desempenho.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
