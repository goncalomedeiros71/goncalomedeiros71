{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Carregar o conjunto de dados\n",
    "df = pd.read_csv('Dados_Trabalho_TP2.csv')\n",
    "\n",
    "# Pré-processar os dados\n",
    "le = LabelEncoder()\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == object:\n",
    "        df[column] = le.fit_transform(df[column])\n",
    "\n",
    "# Dividir o conjunto de dados em recursos (X) e rótulos (y)\n",
    "X = df.drop('Label', axis=1)\n",
    "y = df['Label']\n",
    "\n",
    "# Dividir o conjunto de dados em treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arvores de decisao com K-Fold Cross-Validation\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Carregar o conjunto de dados\n",
    "df = pd.read_csv('Dados_Trabalho_TP2.csv')\n",
    "\n",
    "# Pré-processar os dados\n",
    "le = LabelEncoder()\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == object:\n",
    "        df[column] = le.fit_transform(df[column])\n",
    "\n",
    "# Dividir o conjunto de dados em recursos (X) e rótulos (y)\n",
    "X = df.drop('Label', axis=1)\n",
    "y = df['Label']\n",
    "\n",
    "# Normalizar os dados\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Dividir o conjunto de dados em treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir o número de folds\n",
    "k = 10\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Criar um modelo de Árvore de Decisão\n",
    "model1 = DecisionTreeClassifier(max_depth=12, min_samples_leaf=5, random_state=42)\n",
    "\n",
    "scores1 = []\n",
    "sensitivities = []\n",
    "specificities = []\n",
    "f1_scores = []\n",
    "epsilon = 1e-7  # para evitar divisão por zero\n",
    "prevs_folds = []\n",
    "y_folds = []\n",
    "\n",
    "# Iterar através de cada fold\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    # Treinar o modelo nos dados de treino\n",
    "    model1.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Fazer previsões nos dados de teste\n",
    "    y_pred = model1.predict(X_test_fold)\n",
    "\n",
    "    # Calcular a precisão para este fold\n",
    "    fold_score = accuracy_score(y_test_fold, y_pred) * 100\n",
    "\n",
    "    # Calcular a sensibilidade (recall), especificidade e F1 para este fold\n",
    "    sensitivity = recall_score(y_test_fold, y_pred, average='macro')\n",
    "    cm = confusion_matrix(y_test_fold, y_pred)\n",
    "    specificity = cm[0,0] / (cm[0,0] + cm[0,1] + epsilon)\n",
    "    f1 = f1_score(y_test_fold, y_pred, average='macro')\n",
    "\n",
    "    # Adicionar as métricas deste fold às listas correspondentes\n",
    "    scores1.append(fold_score)\n",
    "    sensitivities.append(sensitivity)\n",
    "    specificities.append(specificity)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    # Adicionar as previsões e os labels do conjunto de teste\n",
    "    prevs_folds.append(y_pred)\n",
    "    y_folds.append(y_test_fold)\n",
    "\n",
    "# Calcular a média e o desvio padrão das precisões\n",
    "mean_accuracy = np.mean(scores1)\n",
    "std_accuracy = np.std(scores1)\n",
    "\n",
    "# Calcular a média das sensibilidades, especificidades e F1\n",
    "mean_sensitivity = np.mean(sensitivities)\n",
    "mean_specificity = np.mean(specificities)\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "\n",
    "print(f\"K-Fold Cross-Validation Scores: {scores1}\")\n",
    "print(f\"Mean Accuracy: {mean_accuracy}\")\n",
    "print(f\"Standard Deviation: {std_accuracy}\")\n",
    "print(f\"Mean Sensitivity: {mean_sensitivity}\")\n",
    "print(f\"Mean Specificity: {mean_specificity}\")\n",
    "print(f\"Mean F1 Score: {mean_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "# Carregar o conjunto de dados \n",
    "df = pd.read_csv('Dados_Trabalho_TP2.csv')\n",
    "\n",
    "# Pré-processar os dados\n",
    "le = LabelEncoder()\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == object:\n",
    "        df[column] = le.fit_transform(df[column])\n",
    "\n",
    "# Dividir o conjunto de dados em recursos (X) e rótulos (y)\n",
    "X = df.drop('Label', axis=1)\n",
    "y = df['Label']\n",
    "\n",
    "# Normalizar os dados\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Dividir o conjunto de dados em treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir o número de folds\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Definindo o modelo SVM com kernel linear\n",
    "kernel = 'linear'\n",
    "svm = SVC(kernel=kernel)\n",
    "\n",
    "# Ajuste de parâmetros utilizando GridSearchCV para evitar overfitting\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000], 'gamma': ['scale', 'auto']}\n",
    "grid = GridSearchCV(svm, param_grid, cv=kf, refit=True, scoring='accuracy', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Avaliar o modelo usando k-fold cross validation\n",
    "scores = []\n",
    "sensitivities = []\n",
    "specificities = []\n",
    "f1_scores = []\n",
    "epsilon = 1e-7  # para evitar divisão por zero\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    best_model.fit(X_train, y_train)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    fold_score = accuracy_score(y_test, y_pred)\n",
    "    scores.append(fold_score)\n",
    "\n",
    "\n",
    "\n",
    "    # Calcular a sensibilidade (recall), especificidade e F1 para este fold\n",
    "    sensitivity = recall_score(y_test, y_pred, average='macro')\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    specificity = cm[0,0] / (cm[0,0] + cm[0,1] + epsilon)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # Adicionar as métricas deste fold às listas correspondentes\n",
    "    sensitivities.append(sensitivity)\n",
    "    specificities.append(specificity)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "mean_score = np.mean(scores) * 100\n",
    "std_score = np.std(scores)\n",
    "\n",
    "# Calcular a média das sensibilidades, especificidades e F1\n",
    "mean_sensitivity = np.mean(sensitivities)\n",
    "mean_specificity = np.mean(specificities)\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "\n",
    "print(f\"Kernel: {kernel}, Best Params: {grid.best_params_}, Mean Accuracy: {mean_score}, Std Accuracy: {std_score}\")\n",
    "print(f\"Mean Sensitivity: {mean_sensitivity}\")\n",
    "print(f\"Mean Specificity: {mean_specificity}\")\n",
    "print(f\"Mean F1 Score: {mean_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#melhor redes Nadam normal\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import recall_score, f1_score, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "epsilon = 1e-10\n",
    "\n",
    "\n",
    "# Carregar o conjunto de dados\n",
    "df = pd.read_csv('Dados_Trabalho_TP2.csv')\n",
    "\n",
    "# Pré-processar os dados\n",
    "le = LabelEncoder()\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == object:\n",
    "        df[column] = le.fit_transform(df[column])\n",
    "\n",
    "# Dividir o conjunto de dados em recursos (X) e rótulos (y)\n",
    "X = df.drop('Label', axis=1)\n",
    "y = df['Label']\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# Dividir o conjunto de dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "optimizers = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "initializers = ['uniform', 'lecun_uniform', 'normal', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "f1score_per_fold = []\n",
    "fold_no = 1\n",
    "epochs_list = [10, 20, 30]\n",
    "batch_sizes = [16, 32, 64]\n",
    "results = []\n",
    "\n",
    "for optimizer in optimizers:\n",
    "    for initializer in initializers:\n",
    "        for train, test in kfold.split(X_train, y_train):\n",
    "            model3 = tf.keras.Sequential([\n",
    "            tf.keras.Input(shape=(17,)),\n",
    "            tf.keras.layers.Dense(units=64, activation=\"relu\", kernel_initializer=initializer),  \n",
    "            tf.keras.layers.Dense(units=48, activation=\"relu\", kernel_initializer=initializer),\n",
    "            tf.keras.layers.Dense(units=32, activation=\"relu\", kernel_initializer=initializer),\n",
    "            tf.keras.layers.Dense(units=32, activation=\"relu\", kernel_initializer=initializer), \n",
    "            tf.keras.layers.Dense(units=16, activation=\"relu\", kernel_initializer=initializer),\n",
    "            tf.keras.layers.Dense(units=16, activation=\"relu\", kernel_initializer=initializer),\n",
    "            tf.keras.layers.Dense(units=16, activation=\"relu\", kernel_initializer=initializer),\n",
    "            tf.keras.layers.Dense(units=16, activation=\"relu\", kernel_initializer=initializer),\n",
    "            tf.keras.layers.Dense(units=16, activation=\"relu\", kernel_initializer=initializer),\n",
    "            tf.keras.layers.Dense(units=16, activation=\"relu\", kernel_initializer=initializer),\n",
    "            tf.keras.layers.Dense(units=16, activation=\"relu\", kernel_initializer=initializer),  \n",
    "            tf.keras.layers.Dense(units=9, activation=\"softmax\", kernel_initializer=initializer)\n",
    "        ])\n",
    "\n",
    "        model3.compile(optimizer=optimizer,\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        # Treina o modelo\n",
    "        history4 = model3.fit(X_train, y_train, epochs=100, verbose=0)\n",
    "\n",
    "        # Avalia o modelo com métricas de generalização\n",
    "        scores3 = model3.evaluate(X_test, y_test, verbose=0)\n",
    "        acc_per_fold.append(scores3[1] * 100)\n",
    "        loss_per_fold.append(scores3[0])\n",
    "\n",
    "        # Faz previsões no conjunto de teste\n",
    "        y_pred = model3.predict(X_test)\n",
    "        y_pred = np.argmax(y_pred, axis=1)  # Converte as previsões de one-hot para rótulos\n",
    "\n",
    "        # Calcula Sensitivity (Recall), Specificity e F1\n",
    "        sensitivity = recall_score(y_test, y_pred, average='macro')\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        specificity = cm[0,0] / (cm[0,0] + cm[0,1] + epsilon)\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "        # Armazena os resultados\n",
    "        results.append({\n",
    "            'optimizer': optimizer,\n",
    "            'initializer': initializer,\n",
    "            'accuracy': scores3[1] * 100,\n",
    "            'loss': scores3[0],\n",
    "            'sensitivity': sensitivity,\n",
    "            'specificity': specificity,\n",
    "            'f1': f1\n",
    "        })\n",
    "\n",
    "# Encontra o resultado com a maior accuracy\n",
    "best_result = max(results, key=lambda x: x['accuracy'])\n",
    "print('Melhor accuracy:', best_result['accuracy'])\n",
    "print('Melhor otimizador:', best_result['optimizer'])\n",
    "print('Melhor inicializador:', best_result['initializer'])\n",
    "print('Sensibilidade:', best_result['sensitivity'])\n",
    "print('Especificidade:', best_result['specificity'])\n",
    "print('F1:', best_result['f1'])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history4.history['accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history4.history['loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-vizinhos-mais-próximos iterando manualmente sobre os parâmetros\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, f1_score\n",
    "\n",
    "\n",
    "data = pd.read_csv('Dados_Trabalho_TP2.csv')\n",
    "\n",
    "\n",
    "# Codificação de variáveis categóricas\n",
    "label_encoders = {}\n",
    "for column in ['Genero', 'Historico_obesidade_familiar', 'FCCAC', 'FCV', 'CCER', 'Fumador', 'CA', 'MCC', 'FAF', 'TUDE', 'CBA', 'TRANS', 'Label']:\n",
    "    le = LabelEncoder()\n",
    "    data[column] = le.fit_transform(data[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Verificação se todas as colunas são numéricas\n",
    "print(data.dtypes)\n",
    "\n",
    "# Separar as features e o target\n",
    "X = data.drop('Label', axis=1)\n",
    "y = data['Label']\n",
    "\n",
    "# Verificar se todas as colunas de X são numéricas antes de escalar\n",
    "print(X.dtypes)\n",
    "\n",
    "# Normalização dos dados\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Configuração do k-fold cross validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Modelo K-vizinhos-mais-próximos\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': list(range(1, 51, 2)),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski'],\n",
    "    'p': [1, 2],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': list(range(20, 51, 10))\n",
    "}\n",
    "\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "best_sensitivity = 0\n",
    "best_specificity = 0\n",
    "best_f1 = 0\n",
    "\n",
    "for k in param_grid['n_neighbors']:\n",
    "    for weight in param_grid['weights']:\n",
    "        for metric in param_grid['metric']:\n",
    "            for p in param_grid['p']:\n",
    "                for algorithm in param_grid['algorithm']:\n",
    "                    for leaf_size in param_grid['leaf_size']:\n",
    "                        knn = KNeighborsClassifier(n_neighbors=k, weights=weight, metric=metric, p=p, algorithm=algorithm, leaf_size=leaf_size)\n",
    "                        for train_index, test_index in kf.split(X):\n",
    "                            X_train, X_test = X[train_index], X[test_index]\n",
    "                            y_train, y_test = y[train_index], y[test_index]\n",
    "                            knn.fit(X_train, y_train)\n",
    "                            y_pred = knn.predict(X_test)\n",
    "                            sensitivity = recall_score(y_test, y_pred, average='macro', zero_division=1)\n",
    "                            cm = confusion_matrix(y_test, y_pred)\n",
    "                            specificity = cm[0,0] / (cm[0,0] + cm[0,1])\n",
    "                            f1 = f1_score(y_test, y_pred, average='macro')\n",
    "                            if sensitivity > best_sensitivity:\n",
    "                                best_sensitivity = sensitivity\n",
    "                            if specificity > best_specificity:\n",
    "                                best_specificity = specificity\n",
    "                            if f1 > best_f1:\n",
    "                                best_f1 = f1\n",
    "                        scores = cross_val_score(knn, X, y, cv=kf, scoring='accuracy')\n",
    "                        mean_score = scores.mean()\n",
    "                        if mean_score > best_score:\n",
    "                            best_score = mean_score\n",
    "                            best_params = {\n",
    "                                'n_neighbors': k,\n",
    "                                'weights': weight,\n",
    "                                'metric': metric,\n",
    "                                'p': p,\n",
    "                                'algorithm': algorithm,\n",
    "                                'leaf_size': leaf_size\n",
    "                            }\n",
    "\n",
    "# Treinar o modelo com os melhores parâmetros\n",
    "knn.set_params(**best_params)\n",
    "knn.fit(X, y)\n",
    "\n",
    "# Avaliação do modelo usando validação cruzada\n",
    "accuracy_scores = cross_val_score(knn, X, y, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Converter a accuracy para uma escala de 1 a 100\n",
    "accuracy_scores = accuracy_scores * 100\n",
    "\n",
    "print(f'Melhores parâmetros: {best_params}')\n",
    "print(f'Média de accuracy: {accuracy_scores.mean()}')\n",
    "print(f'Desvio padrão da accuracy: {accuracy_scores.std()}')\n",
    "print(f'Melhor Sensitivity: {best_sensitivity}')\n",
    "print(f'Melhor Specificity: {best_specificity}')\n",
    "print(f'Melhor F1 Score: {best_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN usando a grid search\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Carregar o dataset\n",
    "data = pd.read_csv('Dados_Trabalho_TP2.csv')\n",
    "\n",
    "# Codificação de variáveis categóricas\n",
    "label_encoders = {}\n",
    "for column in ['Genero', 'Historico_obesidade_familiar', 'FCCAC', 'FCV', 'CCER', 'Fumador', 'CA', 'MCC', 'FAF', 'TUDE', 'CBA', 'TRANS', 'Label']:\n",
    "    le = LabelEncoder()\n",
    "    data[column] = le.fit_transform(data[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Verificação se todas as colunas são numéricas\n",
    "print(data.dtypes)\n",
    "\n",
    "# Separar as features e o target\n",
    "X = data.drop('Label', axis=1)\n",
    "y = data['Label']\n",
    "\n",
    "# Verificar se todas as colunas de X são numéricas antes de escalar\n",
    "print(X.dtypes)\n",
    "\n",
    "# Normalizaçao dos dados\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Configuração do k-fold cross validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Modelo K-vizinhos-mais-próximos\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Ajuste de parâmetros com validação cruzada\n",
    "param_grid = {\n",
    "    'n_neighbors': list(range(1, 51, 2)),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski'],\n",
    "    'p': [1, 2],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': list(range(20, 51, 10))\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=kf, scoring='accuracy')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Melhores parâmetros encontrados\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Treinar o modelo com os melhores parâmetros\n",
    "knn.set_params(**best_params)\n",
    "knn.fit(X, y)\n",
    "\n",
    "# Avaliação do modelo usando validação cruzada\n",
    "accuracy_scores = cross_val_score(knn, X, y, cv=kf, scoring='accuracy')\n",
    "\n",
    "print(f'Melhores parâmetros: {best_params}')\n",
    "print(f'Média de accuracy: {accuracy_scores.mean()}')\n",
    "print(f'Desvio padrão da accuracy: {accuracy_scores.std()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.2 a)\n",
    "\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# Pontuações do modelo SVM\n",
    "scores_svm = scores\n",
    "\n",
    "# Imprime as pontuações de accuracy do modelo SVM\n",
    "print(f\"Pontuações de accuracy do Modelo SVM: {scores_svm}\")\n",
    "\n",
    "# Extrai as pontuações de accuracy da lista 'results'\n",
    "scores_model3 = [result['accuracy'] for result in results]\n",
    "\n",
    "# Imprime as pontuações de accuracy do modelo 3\n",
    "print(f\"Pontuações de accuracy do Modelo 3: {scores_model3}\")\n",
    "\n",
    "# Realiza o teste t\n",
    "t_stat_svm_3, p_val_svm_3 = stats.ttest_ind(scores_svm, scores_model3)\n",
    "\n",
    "# Imprime os resultados\n",
    "print(f\"Modelo SVM vs Modelo 3: t statistic: {t_stat_svm_3:.2f}, p value: {p_val_svm_3:.12f}\")\n",
    "\n",
    "# Verifica se a diferença é significativa\n",
    "if p_val_svm_3 < 0.05:\n",
    "    print(\"Há uma diferença significativa no desempenho dos dois modelos.\")\n",
    "else:\n",
    "    print(\"Não há uma diferença significativa no desempenho dos dois modelos.\")\n",
    "\n",
    "# Identifica o modelo com melhor desempenho\n",
    "if np.mean(scores_svm) > np.mean(scores_model3):\n",
    "    print(\"O Modelo SVM tem melhor desempenho.\")\n",
    "else:\n",
    "    print(\"O Modelo 3 (Rede Neural) tem melhor desempenho.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.2 c) Seleção de Atributos\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Supondo que 'df' é o seu DataFrame e 'Label' é a coluna alvo\n",
    "X = df.drop('Label', axis=1)\n",
    "y = df['Label']\n",
    "\n",
    "# Converte as variáveis categóricas em numéricas\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# Usa a função SelectKBest para selecionar os 10 melhores atributos\n",
    "selector = SelectKBest(score_func=f_classif, k=10)\n",
    "fit = selector.fit(X, y)\n",
    "\n",
    "# Obtém os scores para cada atributo\n",
    "scores = pd.DataFrame(fit.scores_)\n",
    "columns = pd.DataFrame(X.columns)\n",
    "\n",
    "# Junta os dois dataframes para melhor visualização\n",
    "featureScores = pd.concat([columns, scores], axis=1)\n",
    "featureScores.columns = ['Atributo','Score']  # nomeando as colunas\n",
    "print(featureScores.nlargest(10,'Score'))  # imprime os 10 melhores atributos\n",
    "\n",
    "# Usa a função SelectKBest para selecionar os 10 melhores atributos\n",
    "selector = SelectKBest(score_func=f_classif, k=10)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "\n",
    "\n",
    "# divdir os dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalizar os dados\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Treina o modelo com os 10 melhores atributos\n",
    "model1.fit(X_train, y_train)\n",
    "accuracy = model1.score(X_test, y_test)\n",
    "print('accuracy do Modelo 1:', accuracy)\n",
    "\n",
    "# Modelo 2\n",
    "best_model.fit(X_train, y_train)\n",
    "accuracy2 = best_model.score(X_test, y_test)\n",
    "print('accuracy do Modelo 2:', accuracy2)\n",
    "\n",
    "# Modelo 3\n",
    "model3 = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(10,)),  \n",
    "    tf.keras.layers.Dense(units=64, activation=\"relu\", kernel_initializer='normal'),  \n",
    "    tf.keras.layers.Dense(units=48, activation=\"relu\", kernel_initializer='normal'),\n",
    "    tf.keras.layers.Dense(units=32, activation=\"relu\", kernel_initializer='normal'),\n",
    "    tf.keras.layers.Dense(units=32, activation=\"relu\", kernel_initializer='normal'), \n",
    "    tf.keras.layers.Dense(units=16, activation=\"relu\", kernel_initializer='normal'),\n",
    "    tf.keras.layers.Dense(units=16, activation=\"relu\", kernel_initializer='normal'),\n",
    "    tf.keras.layers.Dense(units=16, activation=\"relu\", kernel_initializer='normal'),\n",
    "    tf.keras.layers.Dense(units=16, activation=\"relu\", kernel_initializer='normal'),\n",
    "    tf.keras.layers.Dense(units=16, activation=\"relu\", kernel_initializer='normal'),\n",
    "    tf.keras.layers.Dense(units=16, activation=\"relu\", kernel_initializer='normal'),\n",
    "    tf.keras.layers.Dense(units=16, activation=\"relu\", kernel_initializer='normal'),  \n",
    "    tf.keras.layers.Dense(units=9, activation=\"softmax\", kernel_initializer='normal')\n",
    "])\n",
    "\n",
    "# Compila o modelo\n",
    "model3.compile(optimizer='Nadam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "# Treina o modelo\n",
    "model3.fit(X_train, y_train, verbose=0)\n",
    "\n",
    "# Avalia o modelo\n",
    "loss, accuracy3 = model3.evaluate(X_test, y_test)\n",
    "print('accuracy do Modelo 3:', accuracy3)\n",
    "\n",
    "# Modelo 4\n",
    "knn.fit(X_train, y_train)\n",
    "accuracy4 = knn.score(X_test, y_test)\n",
    "print('accuracy do Modelo 4:', accuracy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow import keras\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "\n",
    "# Carregar o conjunto de dados\n",
    "df = pd.read_csv('Dados_Trabalho_TP2.csv')\n",
    "\n",
    "# Pré-processar os dados\n",
    "le = LabelEncoder()\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == object:\n",
    "        df[column] = le.fit_transform(df[column])\n",
    "\n",
    "# Adicionar o novo preditor\n",
    "df['IMC'] = df['Peso'] / (df['Altura'] ** 2)\n",
    "\n",
    "# Combinar os atributos 'Fumador' e 'CBA' em um único atributo\n",
    "df['Fumador_CBA'] = df['Fumador'] + df['CBA']\n",
    "\n",
    "atributos = ['Genero', 'Idade', 'Altura', 'Peso', 'Historico_obesidade_familiar', 'FCCAC', 'FCV', 'NRP', 'CCER', 'Fumador', 'CA', 'MCC', 'FAF', 'TUDE', 'CBA', 'TRANS']\n",
    "\n",
    "atributos_com_IMC_Fumador_CBA = atributos + ['IMC', 'Fumador_CBA']\n",
    "\n",
    "X = df[atributos_com_IMC_Fumador_CBA]  # Agora inclui os novos preditores IMC e Fumador_CBA\n",
    "y = df['Label']\n",
    "\n",
    "# Dividir o conjunto de dados em treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalizar os dados\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Definindo o modelo SVM com kernel linear\n",
    "kernel = 'linear'\n",
    "svm = SVC(kernel=kernel)\n",
    "\n",
    "# Ajuste de parâmetros utilizando GridSearchCV para evitar overfitting\n",
    "param_grid = {'C': [ 100], 'gamma': ['scale']}\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid = GridSearchCV(svm, param_grid, cv=kf, refit=True, scoring='accuracy', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Avaliar o modelo no conjunto de teste\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred)\n",
    "print('accuracy do Modelo SVM com IMC e Fumador_CBA:', accuracy_svm)\n",
    "\n",
    "\n",
    "# Treinar o modelo 3 com os atributos selecionados\n",
    "model3 = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(18,)),  \n",
    "    tf.keras.layers.Dense(units=64, activation=\"relu\", kernel_initializer='normal'),  \n",
    "    tf.keras.layers.Dense(units=48, activation=\"relu\", kernel_initializer='normal'),\n",
    "    tf.keras.layers.Dense(units=32, activation=\"relu\", kernel_initializer='normal'),\n",
    "    tf.keras.layers.Dense(units=32, activation=\"relu\", kernel_initializer='normal'), \n",
    "    tf.keras.layers.Dense(units=16, activation=\"relu\", kernel_initializer='normal'),\n",
    "    tf.keras.layers.Dense(units=16, activation=\"relu\", kernel_initializer='normal'),\n",
    "    tf.keras.layers.Dense(units=16, activation=\"relu\", kernel_initializer='normal'),\n",
    "    tf.keras.layers.Dense(units=16, activation=\"relu\", kernel_initializer='normal'),\n",
    "    tf.keras.layers.Dense(units=16, activation=\"relu\", kernel_initializer='normal'),\n",
    "    tf.keras.layers.Dense(units=16, activation=\"relu\", kernel_initializer='normal'),\n",
    "    tf.keras.layers.Dense(units=16, activation=\"relu\", kernel_initializer='normal'),  \n",
    "    tf.keras.layers.Dense(units=9, activation=\"softmax\", kernel_initializer='normal')\n",
    "])\n",
    "\n",
    "# Compila o modelo\n",
    "model3.compile(optimizer='Nadam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "# Treina o modelo\n",
    "model3.fit(X_train, y_train, epochs=100, verbose=0)\n",
    "\n",
    "# Avalia o modelo\n",
    "loss, accuracy3 = model3.evaluate(X_test, y_test)\n",
    "print('accuracy do Modelo Redes com IMC e Fumador_CBA:', accuracy3)\n",
    "\n",
    "\n",
    "\n",
    "# Pontuações do modelo SVM\n",
    "scores_svm = scores\n",
    "\n",
    "# Extrai as pontuações de accuracy da lista 'results'\n",
    "scores_model3 = [result['accuracy'] for result in results]\n",
    "\n",
    "# Realiza o teste t de uma amostra para o modelo SVM\n",
    "t_stat_svm, p_value_svm = stats.ttest_1samp(scores_svm, accuracy_svm)\n",
    "\n",
    "# Realiza o teste t de uma amostra para o modelo 3\n",
    "t_stat_model3, p_value_model3 = stats.ttest_1samp(scores_model3, accuracy3)\n",
    "\n",
    "# Imprime o valor p para o modelo SVM\n",
    "print(\"Valor p para o modelo SVM: \", p_value_svm)\n",
    "\n",
    "# Imprime o valor p para o modelo 3\n",
    "print(\"Valor p para o modelo 3: \", p_value_model3)\n",
    "\n",
    "# Verifica se a diferença é significativa para o modelo SVM\n",
    "if p_value_svm < 0.05:\n",
    "    print(\"Existe uma diferença significativa no desempenho do modelo SVM com a utilização do IMC e Fumador_CBA.\")\n",
    "else:\n",
    "    print(\"Não existe uma diferença significativa no desempenho do modelo SVM com a IMC e Fumador_CBA.\")\n",
    "\n",
    "# Verifica se a diferença é significativa para o modelo 3\n",
    "if p_value_model3 < 0.05:\n",
    "    print(\"Existe uma diferença significativa no desempenho do modelo de redes com a utilização do IMC e Fumador_CBA.\")\n",
    "else:\n",
    "    print(\"Não existe uma diferença significativa no desempenho do modelo de redes com a utilização do IMC e Fumador_CBA.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
